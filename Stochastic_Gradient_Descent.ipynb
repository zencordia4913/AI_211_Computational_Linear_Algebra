{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the dataset...\n",
      "Epoch 1:  average error:  0.39783686164282217\n",
      "Epoch 2:  average error:  0.35793975094456276\n",
      "Epoch 3:  average error:  0.3472158132816834\n",
      "Epoch 4:  average error:  0.34122691948359907\n",
      "Epoch 5:  average error:  0.33862662071397737\n",
      "Epoch 6:  average error:  0.3366901032672609\n",
      "Epoch 7:  average error:  0.33590798209444184\n",
      "Epoch 8:  average error:  0.33512260032771424\n",
      "Epoch 9:  average error:  0.3347334538434814\n",
      "Epoch 10:  average error:  0.3345647094322917\n",
      "Epoch 11:  average error:  0.33436609320639915\n",
      "Epoch 12:  average error:  0.3342359870398876\n",
      "Epoch 13:  average error:  0.3341945043567499\n",
      "Epoch 14:  average error:  0.334162476795611\n",
      "Epoch 15:  average error:  0.3341516812508895\n",
      "Epoch 16:  average error:  0.33408061206897266\n",
      "Epoch 17:  average error:  0.33410187099740063\n",
      "Epoch 18:  average error:  0.33407017483541956\n",
      "Epoch 19:  average error:  0.33406307514509703\n",
      "Converged. Stopping training for epoch 18, average error:  0.33406307514509703\n",
      "\n",
      "\n",
      "Here is the cache and parameters after the last epoch: \n",
      "\n",
      "\n",
      "\u001b[1mFor layer \u001b[0m 1  :\n",
      "\n",
      "Outputs:\n",
      "\u001b[1mx:\u001b[0m\n",
      "[6.5 3.  5.2 2. ]\n",
      "\u001b[1mA:\u001b[0m\n",
      "[[-1.79690315  0.47083382 -1.5335708  -0.51038365 -1.33485889  0.1965557\n",
      "  -0.54453537 -0.11537932 -1.55352873  0.7307819  -1.6065667   0.69918317]\n",
      " [ 0.1523859   0.1425106   0.36271268 -1.88025516  0.69115672 -0.51034758\n",
      "   1.21563484 -0.14506043 -0.3710177  -1.90444045 -0.95326522  0.18739317]\n",
      " [-1.30446234 -1.98768859 -0.84454738  0.84007577  1.26148301 -0.67637564\n",
      "  -1.54062699 -0.33175764  0.18899691 -0.36177472  1.78093654 -0.12782386]\n",
      " [ 1.65663739 -0.46174574  0.09062655 -0.59285996 -0.87571025 -0.26301254\n",
      "  -0.27813588 -0.59940691 -0.5475692   0.77914695  1.9782415  -0.60834205]]\n",
      "\u001b[1mb:\u001b[0m\n",
      "[-0.32359539  0.52511554  3.1899496   0.08418176  0.87850086  1.5446708\n",
      "  0.34594717  0.92820288  0.60898024  0.82972457 -1.09695665  0.12163852]\n",
      "\u001b[1mu:\u001b[0m\n",
      "[-14.69264886  -7.77152048 -13.09046544  -5.77558513  -1.7948215\n",
      "  -4.29660911  -8.46010847  -4.10910042 -11.32334437  -1.28617364\n",
      "  -0.08512618   3.22555792]\n",
      "\u001b[1mv:\u001b[0m\n",
      "[-15.01624433  -7.24640495  -9.90051584  -5.69140337  -0.91632064\n",
      "  -2.75193831  -8.11416131  -3.18089754 -10.71436413  -0.45644907\n",
      "  -1.18208283   3.34719712]\n",
      "\u001b[1mz:\u001b[0m\n",
      "[-0.15016244 -0.07246405 -0.09900516 -0.05691403 -0.00916321 -0.02751938\n",
      " -0.08114161 -0.03180898 -0.10714364 -0.00456449 -0.01182083  3.34719712]\n",
      "\u001b[1mL:\u001b[0m\n",
      "None\n",
      "\n",
      "Gradients:\n",
      "\u001b[1mdL/dz:\u001b[0m\n",
      "None\n",
      "\u001b[1mdu/dz:\u001b[0m\n",
      "[[ 7.37670642e-04]\n",
      " [ 2.49588732e-05]\n",
      " [-3.05476642e-05]\n",
      " [ 1.14711764e-04]\n",
      " [ 1.01632168e-04]\n",
      " [ 9.54341615e-05]\n",
      " [ 3.78388558e-04]\n",
      " [ 1.17704613e-04]\n",
      " [-9.35703655e-05]\n",
      " [-3.82441179e-04]\n",
      " [-2.57504226e-04]\n",
      " [ 4.50879603e-04]]\n",
      "\u001b[1mdz/dv:\u001b[0m\n",
      "[[ 7.37670642e-04]\n",
      " [ 2.49588732e-05]\n",
      " [-3.05476642e-07]\n",
      " [ 1.14711764e-04]\n",
      " [ 1.01632168e-04]\n",
      " [ 9.54341615e-05]\n",
      " [ 3.78388558e-04]\n",
      " [ 1.17704613e-04]\n",
      " [-9.35703655e-07]\n",
      " [-3.82441179e-06]\n",
      " [-2.57504226e-06]\n",
      " [ 4.50879603e-04]]\n",
      "\u001b[1mdv/du:\u001b[0m\n",
      "[[-1.10770426e-04]\n",
      " [-1.80862102e-06]\n",
      " [ 3.02437633e-08]\n",
      " [-6.52870923e-06]\n",
      " [-9.31276536e-07]\n",
      " [-2.62628925e-06]\n",
      " [-3.07030580e-05]\n",
      " [-3.74406313e-06]\n",
      " [ 1.00254697e-07]\n",
      " [ 1.74564922e-08]\n",
      " [ 3.04391323e-08]\n",
      " [ 1.50918291e-03]]\n",
      "\u001b[1mdv/db:\u001b[0m\n",
      "[[-1.10770426e-04]\n",
      " [-1.80862102e-06]\n",
      " [ 3.02437633e-08]\n",
      " [-6.52870923e-06]\n",
      " [-9.31276536e-07]\n",
      " [-2.62628925e-06]\n",
      " [-3.07030580e-05]\n",
      " [-3.74406313e-06]\n",
      " [ 1.00254697e-07]\n",
      " [ 1.74564922e-08]\n",
      " [ 3.04391323e-08]\n",
      " [ 1.50918291e-03]]\n",
      "\u001b[1mdu/dx:\u001b[0m\n",
      "[[ 1.27436586e-03]\n",
      " [ 2.41779301e-04]\n",
      " [-1.11102642e-06]\n",
      " [-1.08458628e-03]]\n",
      "\u001b[1mdu/dA:\u001b[0m\n",
      "[[-7.20007769e-04 -1.17560366e-05  1.96584461e-07 -4.24366100e-05\n",
      "  -6.05329749e-06 -1.70708801e-05 -1.99569877e-04 -2.43364103e-05\n",
      "   6.51655529e-07  1.13467199e-07  1.97854360e-07  9.80968891e-03]\n",
      " [-3.32311278e-04 -5.42586306e-06  9.07312899e-08 -1.95861277e-05\n",
      "  -2.79382961e-06 -7.87886774e-06 -9.21091739e-05 -1.12321894e-05\n",
      "   3.00764090e-07  5.23694766e-08  9.13173970e-08  4.52754873e-03]\n",
      " [-5.76006215e-04 -9.40482930e-06  1.57267569e-07 -3.39492880e-05\n",
      "  -4.84263799e-06 -1.36567041e-05 -1.59655901e-04 -1.94691283e-05\n",
      "   5.21324423e-07  9.07737595e-08  1.58283488e-07  7.84775113e-03]\n",
      " [-2.21540852e-04 -3.61724204e-06  6.04875266e-08 -1.30574185e-05\n",
      "  -1.86255307e-06 -5.25257850e-06 -6.14061159e-05 -7.48812626e-06\n",
      "   2.00509393e-07  3.49129844e-08  6.08782647e-08  3.01836582e-03]]\n",
      "\u001b[1mdL/dA:\u001b[0m\n",
      "[[-5.31128594e-07 -2.93417427e-10 -6.00519611e-12 -4.86797841e-09\n",
      "  -6.15209749e-10 -1.62914513e-09 -7.55149578e-08 -2.86450775e-09\n",
      "  -6.09756460e-11 -4.33945295e-11 -5.09483339e-11  4.42298865e-06]\n",
      " [-2.45136274e-07 -1.35423428e-10 -2.77162897e-12 -2.24675927e-09\n",
      "  -2.83942961e-10 -7.51913137e-10 -3.48530574e-08 -1.32208050e-09\n",
      "  -2.81426058e-11 -2.00282444e-11 -2.35146157e-11  2.04137938e-06]\n",
      " [-4.24902875e-07 -2.34733942e-10 -4.80415689e-12 -3.89438273e-09\n",
      "  -4.92167799e-10 -1.30331610e-09 -6.04119662e-08 -2.29160620e-09\n",
      "  -4.87805168e-11 -3.47156236e-11 -4.07586671e-11  3.53839092e-06]\n",
      " [-1.63424183e-07 -9.02822852e-11 -1.84775265e-12 -1.49783951e-09\n",
      "  -1.89295307e-10 -5.01275424e-10 -2.32353716e-08 -8.81387000e-10\n",
      "  -1.87617372e-11 -1.33521629e-11 -1.56764104e-11  1.36091958e-06]]\n",
      "\u001b[1mdL/db:\u001b[0m\n",
      "[-8.17120913e-08 -4.51411426e-11 -9.23876324e-13 -7.48919755e-10\n",
      " -9.46476537e-11 -2.50637712e-10 -1.16176858e-08 -4.40693500e-10\n",
      " -9.38086862e-12 -6.67608146e-12 -7.83820522e-12  6.80459792e-07]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mFor layer \u001b[0m 2  :\n",
      "\n",
      "Outputs:\n",
      "\u001b[1mx:\u001b[0m\n",
      "[-0.15016244 -0.07246405 -0.09900516 -0.05691403 -0.00916321 -0.02751938\n",
      " -0.08114161 -0.03180898 -0.10714364 -0.00456449 -0.01182083  3.34719712]\n",
      "\u001b[1mA:\u001b[0m\n",
      "[[ 1.46907213 -0.88006076 -0.79238872 -1.17555992 -1.58808083 -0.42544249]\n",
      " [ 1.44028194 -1.91507877  0.85163752  0.76701883  0.56668466  0.73199384]\n",
      " [-1.1097813   0.76115512 -0.54970152 -2.46136113  0.05705919 -0.84608121]\n",
      " [ 1.35532242 -0.28654062  0.04536888  0.3878083  -0.26909291 -0.10508852]\n",
      " [ 0.84883012 -1.07560344 -0.70137969 -0.96485532  1.40847591 -0.6991822 ]\n",
      " [-0.14453566 -0.60073029 -0.30830284 -0.7379202   0.46328494 -1.04129178]\n",
      " [-1.35671412 -0.55877965 -0.79860301 -1.33950345 -0.32191921 -1.66162425]\n",
      " [ 1.72617496 -1.49274528 -0.64644253 -0.40416995  1.68487957 -1.73087278]\n",
      " [ 0.13577411  1.03674213 -0.3102085  -1.17226832 -0.1093876   1.52486866]\n",
      " [-0.61376468  1.39887481  0.25883956  1.23279922  0.20314804 -0.48365508]\n",
      " [ 1.63753399  0.22571227  0.16757957  0.80716032  0.8744252   0.81234452]\n",
      " [-1.02419947 -0.63145325 -0.63646046 -0.65224195 -0.76802072 -0.55206172]]\n",
      "\u001b[1mb:\u001b[0m\n",
      "[ 0.40134429  0.49802573  0.00267964 -0.40045139  1.29378787 -0.06822923]\n",
      "\u001b[1mu:\u001b[0m\n",
      "[-3.7001564  -1.90267457 -1.89094232 -1.57949875 -2.41636342 -1.69309807]\n",
      "\u001b[1mv:\u001b[0m\n",
      "[-3.29881236 -1.40465146 -1.88826686 -1.97995036 -1.12257966 -1.76132731]\n",
      "\u001b[1mz:\u001b[0m\n",
      "[-0.03298812 -0.01404651 -0.01888267 -0.0197995  -0.0112258  -0.01761327]\n",
      "\u001b[1mL:\u001b[0m\n",
      "None\n",
      "\n",
      "Gradients:\n",
      "\u001b[1mdL/dz:\u001b[0m\n",
      "None\n",
      "\u001b[1mdu/dz:\u001b[0m\n",
      "[[-0.02740233]\n",
      " [ 0.01365332]\n",
      " [ 0.01487143]\n",
      " [-0.03294044]\n",
      " [ 0.01913148]\n",
      " [-0.00399552]]\n",
      "\u001b[1mdz/dv:\u001b[0m\n",
      "[[-2.74023314e-04]\n",
      " [ 1.36533210e-02]\n",
      " [ 1.48714269e-02]\n",
      " [-3.29404443e-04]\n",
      " [ 1.91314786e-02]\n",
      " [-3.99551774e-05]]\n",
      "\u001b[1mdv/du:\u001b[0m\n",
      "[[ 9.03951497e-06]\n",
      " [-1.91781573e-04]\n",
      " [-2.80812225e-04]\n",
      " [ 6.52204445e-06]\n",
      " [-2.14766087e-04]\n",
      " [ 7.03741451e-07]]\n",
      "\u001b[1mdv/db:\u001b[0m\n",
      "[[ 9.03951497e-06]\n",
      " [-1.91781573e-04]\n",
      " [-2.80812225e-04]\n",
      " [ 6.52204445e-06]\n",
      " [-2.14766087e-04]\n",
      " [ 7.03741451e-07]]\n",
      "\u001b[1mdu/dx:\u001b[0m\n",
      "[[ 7.37670642e-04]\n",
      " [ 2.49588732e-05]\n",
      " [-3.05476642e-05]\n",
      " [ 1.14711764e-04]\n",
      " [ 1.01632168e-04]\n",
      " [ 9.54341615e-05]\n",
      " [ 3.78388558e-04]\n",
      " [ 1.17704613e-04]\n",
      " [-9.35703655e-05]\n",
      " [-3.82441179e-04]\n",
      " [-2.57504226e-04]\n",
      " [ 4.50879603e-04]]\n",
      "\u001b[1mdu/dA:\u001b[0m\n",
      "[[-1.35739565e-06  2.87983896e-05  4.21674498e-05 -9.79366130e-07\n",
      "   3.22498004e-05 -1.05675536e-07]\n",
      " [-6.55039860e-07  1.38972694e-05  2.03487910e-05 -4.72613752e-07\n",
      "   1.55628203e-05 -5.09959553e-08]\n",
      " [-8.94958611e-07  1.89873650e-05  2.78018588e-05 -6.45716044e-07\n",
      "   2.12629505e-05 -6.96740338e-08]\n",
      " [-5.14475259e-07  1.09150629e-05  1.59821564e-05 -3.71195858e-07\n",
      "   1.22232043e-05 -4.00527646e-08]\n",
      " [-8.28309415e-08  1.75733414e-06  2.57314038e-06 -5.97628395e-08\n",
      "   1.96794598e-06 -6.44852817e-09]\n",
      " [-2.48761875e-07  5.27771056e-06  7.72777919e-06 -1.79482640e-07\n",
      "   5.91023021e-06 -1.93665306e-08]\n",
      " [-7.33480826e-07  1.55614662e-05  2.27855569e-05 -5.29209208e-07\n",
      "   1.74264667e-05 -5.71027165e-08]\n",
      " [-2.87537709e-07  6.10037533e-06  8.93234916e-06 -2.07459552e-07\n",
      "   6.83148918e-06 -2.23852945e-08]\n",
      " [-9.68526549e-07  2.05481760e-05  3.00872443e-05 -6.98795591e-07\n",
      "   2.30108206e-05 -7.54014215e-08]\n",
      " [-4.12607824e-08  8.75385214e-07  1.28176480e-06 -2.97698115e-08\n",
      "   9.80297816e-07 -3.21222134e-09]\n",
      " [-1.06854554e-07  2.26701704e-06  3.31943309e-06 -7.70959674e-08\n",
      "   2.53871303e-06 -8.31880683e-09]\n",
      " [ 3.02570385e-05 -6.41930728e-04 -9.39933871e-04  2.18305684e-05\n",
      "  -7.18864427e-04  2.35556136e-06]]\n",
      "\u001b[1mdL/dA:\u001b[0m\n",
      "[[ 3.71958056e-08  3.93193656e-07  6.27090146e-07  3.22607555e-08\n",
      "   6.16986364e-07  4.22228478e-10]\n",
      " [ 1.79496193e-08  1.89743879e-07  3.02615557e-07  1.55681070e-08\n",
      "   2.97739764e-07  2.03755244e-10]\n",
      " [ 2.45239525e-08  2.59240589e-07  4.13453310e-07  2.12701734e-08\n",
      "   4.06791681e-07  2.78383838e-10]\n",
      " [ 1.40978216e-08  1.49026857e-07  2.37677471e-07  1.22273565e-08\n",
      "   2.33847971e-07  1.60031532e-10]\n",
      " [ 2.26976091e-09  2.39934470e-08  3.82662690e-08  1.96861449e-09\n",
      "   3.76497164e-08  2.57652087e-11]\n",
      " [ 6.81665535e-09  7.20582763e-08  1.14923103e-07  5.91223790e-09\n",
      "   1.13071443e-07  7.73793164e-11]\n",
      " [ 2.00990847e-08  2.12465693e-07  3.38853743e-07  1.74323864e-08\n",
      "   3.33394075e-07  2.28154917e-10]\n",
      " [ 7.87920361e-09  8.32903825e-08  1.32836777e-07  6.83380981e-09\n",
      "   1.30696489e-07  8.94408414e-11]\n",
      " [ 2.65398855e-08  2.80550843e-07  4.47440253e-07  2.30186373e-08\n",
      "   4.40231021e-07  3.01267718e-10]\n",
      " [ 1.13064163e-09  1.19519153e-08  1.90616715e-08  9.80630820e-10\n",
      "   1.87545467e-08  1.28344873e-11]\n",
      " [ 2.92806390e-09  3.09523112e-08  4.93647064e-08  2.53957542e-09\n",
      "   4.85693339e-08  3.32379403e-11]\n",
      " [-8.29113396e-07 -8.76448627e-06 -1.39781578e-05 -7.19108623e-07\n",
      "  -1.37529394e-05 -9.41168719e-09]]\n",
      "\u001b[1mdL/db:\u001b[0m\n",
      "[-2.47703785e-07 -2.61845537e-06 -4.17607847e-06 -2.14839042e-07\n",
      " -4.10879279e-06 -2.81181145e-09]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mFor layer \u001b[0m 3  :\n",
      "\n",
      "Outputs:\n",
      "\u001b[1mx:\u001b[0m\n",
      "[-0.03298812 -0.01404651 -0.01888267 -0.0197995  -0.0112258  -0.01761327]\n",
      "\u001b[1mA:\u001b[0m\n",
      "[[ 0.30481619 -1.66698805  1.16489031]\n",
      " [ 0.57288508  1.22920689 -0.00851866]\n",
      " [ 0.18997485 -0.34237708 -1.0739855 ]\n",
      " [-0.58490903 -1.71291701  1.05656419]\n",
      " [-2.22116282  1.87335285 -1.49305899]\n",
      " [ 1.53409285 -0.77560728  0.67005121]]\n",
      "\u001b[1mb:\u001b[0m\n",
      "[-2.23961769 -2.37987942 -2.24278809]\n",
      "\u001b[1mu:\u001b[0m\n",
      "[-0.01217584  0.07075412 -0.03402538]\n",
      "\u001b[1mv:\u001b[0m\n",
      "[-2.2441376  -2.30168523 -2.2916839 ]\n",
      "\u001b[1mz:\u001b[0m\n",
      "[0.33777144 0.33392062 0.33458829]\n",
      "\u001b[1mL:\u001b[0m\n",
      "[0.33777144 0.33392062 0.33458829]\n",
      "\n",
      "Gradients:\n",
      "\u001b[1mdL/dz:\u001b[0m\n",
      "[ 0.33777144  0.33392062 -0.66541171]\n",
      "\u001b[1mdu/dz:\u001b[0m\n",
      "None\n",
      "\u001b[1mdz/dv:\u001b[0m\n",
      "[ 0.02266601  0.02228095 -0.04444397]\n",
      "\u001b[1mdv/du:\u001b[0m\n",
      "[[ 0.00765593]\n",
      " [ 0.00744007]\n",
      " [-0.01487043]]\n",
      "\u001b[1mdv/db:\u001b[0m\n",
      "[[ 0.00765593]\n",
      " [ 0.00744007]\n",
      " [-0.01487043]]\n",
      "\u001b[1mdu/dx:\u001b[0m\n",
      "[[-0.02740233]\n",
      " [ 0.01365332]\n",
      " [ 0.01487143]\n",
      " [-0.03294044]\n",
      " [ 0.01913148]\n",
      " [-0.00399552]]\n",
      "\u001b[1mdu/dA:\u001b[0m\n",
      "[[-2.52554764e-04 -2.45433906e-04  4.90547675e-04]\n",
      " [-1.07539132e-04 -1.04507034e-04  2.08877752e-04]\n",
      " [-1.44564388e-04 -1.40488352e-04  2.80793454e-04]\n",
      " [-1.51583612e-04 -1.47309667e-04  2.94427187e-04]\n",
      " [-8.59439123e-05 -8.35206978e-05  1.66932454e-04]\n",
      " [-1.34845985e-04 -1.31043962e-04  2.61916994e-04]]\n",
      "\u001b[1mdL/dA:\u001b[0m\n",
      "[[-2.52554764e-04 -2.45433906e-04  4.90547675e-04]\n",
      " [-1.07539132e-04 -1.04507034e-04  2.08877752e-04]\n",
      " [-1.44564388e-04 -1.40488352e-04  2.80793454e-04]\n",
      " [-1.51583612e-04 -1.47309667e-04  2.94427187e-04]\n",
      " [-8.59439123e-05 -8.35206978e-05  1.66932454e-04]\n",
      " [-1.34845985e-04 -1.31043962e-04  2.61916994e-04]]\n",
      "\u001b[1mdL/db:\u001b[0m\n",
      "[[-2.52554764e-04 -2.45433906e-04  4.90547675e-04]\n",
      " [-1.07539132e-04 -1.04507034e-04  2.08877752e-04]\n",
      " [-1.44564388e-04 -1.40488352e-04  2.80793454e-04]\n",
      " [-1.51583612e-04 -1.47309667e-04  2.94427187e-04]\n",
      " [-8.59439123e-05 -8.35206978e-05  1.66932454e-04]\n",
      " [-1.34845985e-04 -1.31043962e-04  2.61916994e-04]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, i, input_size, output_size):\n",
    "        self.input_size, self.output_size = input_size, output_size\n",
    "        self.i = i\n",
    "        self.output, self.grad = self.initialize_output_param()\n",
    "        \n",
    "    def initialize_output_param(self):\n",
    "        # Xavier initialization for weights\n",
    "        weight_variance = 2.0 / (self.input_size + self.output_size)\n",
    "        initialized_weights = np.random.normal(0, np.sqrt(weight_variance), (self.input_size, self.output_size))\n",
    "        initialized_weights = np.abs(initialized_weights)  # Ensure positive weights\n",
    "\n",
    "        # Initialized parameters\n",
    "        output = {\n",
    "            \"x\": None,\n",
    "            \"A\": np.random.randn(self.input_size, self.output_size),\n",
    "            \"b\": np.random.randn(self.output_size),\n",
    "            \"u\": None,\n",
    "            \"v\": None,\n",
    "            \"z\": None,\n",
    "            \"L\": None\n",
    "        }\n",
    "        \n",
    "        grad = {\n",
    "            \"dL/dz\": None,\n",
    "            \"du/dz\": None,\n",
    "            \"dz/dv\": None,\n",
    "            \"dv/du\": None,\n",
    "            \"dv/db\": None,\n",
    "            \"du/dx\": None,\n",
    "            \"du/dA\": None,\n",
    "            \"dL/dA\": None,\n",
    "            \"dL/db\": None\n",
    "        }\n",
    "        \n",
    "        return output, grad\n",
    "    \n",
    "    def ReLu(self, x, direction, max_layer = None, a = 0.01):\n",
    "        if direction == \"forward\":\n",
    "            return np.maximum(a*x, x) # Applying ReLU element-wise\n",
    "        \n",
    "        elif direction == \"backward\":\n",
    "                if max_layer == True:\n",
    "                    return np.where(x > 0, 1, 0)\n",
    "                elif max_layer == False:\n",
    "                    return np.maximum(a*x, x)\n",
    "            \n",
    "    def sigmoid(self, x, direction, max_layer = None, a_L = 0.3):\n",
    "        if direction == \"forward\":\n",
    "            return 1 / (1 + np.exp(-a_L * x)) # Applying ReLU element-wise\n",
    "        \n",
    "        elif direction == \"backward\":\n",
    "                if max_layer == True:\n",
    "                    return a_L * self.grad[\"dL/dz\"] * x * (1 - x)\n",
    "                elif max_layer == False:\n",
    "                    return None\n",
    "\n",
    "                \n",
    "    def forward_prop(self, x, layer = None):\n",
    "        self.output[\"x\"] = x\n",
    "        self.output[\"u\"] = np.matmul(x.T, self.output[\"A\"])\n",
    "        self.output[\"v\"] = self.output[\"b\"] + self.output[\"u\"]\n",
    "        \n",
    "        if layer == \"output\":\n",
    "            self.output[\"z\"] = self.sigmoid(self.output[\"v\"], \"forward\")\n",
    "            self.output[\"L\"] = self.output[\"z\"]\n",
    "        else:\n",
    "            self.output[\"z\"] = self.ReLu(self.output[\"v\"], \"forward\")\n",
    "\n",
    "        return self.output[\"z\"].T\n",
    "    \n",
    "    def backward_prop(self, layer, max_layer, y = None, dudz = None, A = None):\n",
    "        if layer == max_layer:\n",
    "            self.grad[\"dL/dz\"] = (self.output[\"L\"] - y.T)\n",
    "            self.grad[\"dz/dv\"] = self.sigmoid(self.output[\"L\"], \"backward\", True).T\n",
    "            self.grad[\"dv/db\"] = self.grad[\"dz/dv\"].reshape(-1,1)*self.output[\"z\"].reshape(-1,1)\n",
    "            \n",
    "        else:\n",
    "            self.grad[\"du/dz\"] = dudz\n",
    "            self.grad[\"dz/dv\"] = self.ReLu(self.grad[\"du/dz\"], \"backward\", False)\n",
    "            self.grad[\"dv/db\"] = self.grad[\"dz/dv\"].reshape(-1,1)*self.output[\"z\"].reshape(-1,1)\n",
    "\n",
    "        \n",
    "        #print(self.grad[\"dv/db\"])   \n",
    "        self.grad[\"dv/du\"] = self.grad[\"dz/dv\"].reshape(-1,1)*self.output[\"z\"].reshape(-1,1)\n",
    "        self.grad[\"du/dA\"] = (self.grad[\"dv/du\"] * self.output[\"x\"].T).T\n",
    "        self.grad[\"du/dx\"] = np.dot(self.output[\"A\"], self.grad[\"dv/du\"].reshape(-1, 1))\n",
    "        return self.grad[\"du/dx\"], self.grad[\"du/dA\"]\n",
    "    \n",
    "    def update_weights_bias(self, layer, max_layer, mult_grad, learning_rate = 0.003):\n",
    "        if layer == max_layer:\n",
    "            self.output[\"A\"] -= learning_rate * (self.grad[\"du/dA\"])\n",
    "            self.output[\"b\"] -= learning_rate * (self.grad[\"dv/db\"]).T.flatten()\n",
    "            self.grad[\"dL/dA\"] = (self.grad[\"du/dA\"])\n",
    "            self.grad[\"dL/db\"] = (self.grad[\"du/dA\"])\n",
    "            return self.grad[\"du/dx\"]\n",
    "        else:\n",
    "            #print(self.grad[\"dv/db\"].shape)\n",
    "            #print(mult_grad.shape)\n",
    "            self.output[\"A\"] -= learning_rate * ((mult_grad * self.grad[\"du/dA\"].T).T)\n",
    "            self.output[\"b\"] -= learning_rate * ((mult_grad * self.grad[\"dv/db\"])).T.flatten()\n",
    "            self.grad[\"dL/dA\"] = ((mult_grad * self.grad[\"du/dA\"].T).T)\n",
    "            self.grad[\"dL/db\"] = ((mult_grad * self.grad[\"dv/db\"])).T.flatten()\n",
    "            return self.grad[\"du/dx\"]\n",
    "    \n",
    "    \n",
    "class Neural_Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []    \n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        i = 1\n",
    "        max = len(self.layers)\n",
    "        # Perform a forward pass for all the layers\n",
    "        for layer in self.layers:\n",
    "            if i == max:\n",
    "                x = layer.forward_prop(x, \"output\")\n",
    "            else:\n",
    "                x = layer.forward_prop(x)\n",
    "            #print(f\"\\n\\033[1mOutput for layer {i}: \\033[0m\")\n",
    "            #print(x)\n",
    "            i += 1\n",
    "        return x\n",
    "    \n",
    "    def backward_pass(self, y, grad = None, grad_mult = None):\n",
    "        \n",
    "        i = len(self.layers) + 1\n",
    "        #Perform a backward pass for all the layers\n",
    "        for layer in reversed(self.layers):\n",
    "            grad, grad_2 = layer.backward_prop(i, len(self.layers) + 1, y, grad)\n",
    "            grad_mult = layer.update_weights_bias(i, len(self.layers) + 1, grad_mult, learning_rate = 1.0)\n",
    "            #print(f\"\\n\\033[1mGradient for layer {i-1}: \\033[0m\")\n",
    "            #print(grad_2)\n",
    "            i -= 1\n",
    "        return None\n",
    "\n",
    "def print_results(network):\n",
    "    i = 0\n",
    "    print(\"\\n\\nHere is the cache and parameters after the last epoch: \")\n",
    "    for layer in network.layers:\n",
    "        print(\"\\n\\n\\033[1mFor layer \\033[0m\", i+1, \" :\")\n",
    "        print(\"\")\n",
    "        \n",
    "        if hasattr(layer, 'grad') and isinstance(layer.grad, dict):\n",
    "            print(\"Outputs:\")\n",
    "            if not layer.output: \n",
    "                print(\"No outputs\")\n",
    "            else:\n",
    "                for key, value in layer.output.items():\n",
    "                    print(f\"\\033[1m{key}:\\033[0m\")\n",
    "                    print(value) \n",
    "                    \n",
    "            print(\"\\nGradients:\")\n",
    "            if not layer.grad:  \n",
    "                print(\"No gradients\")\n",
    "            else:\n",
    "                for key, value in layer.grad.items():\n",
    "                    print(f\"\\033[1m{key}:\\033[0m\")\n",
    "                    print(value) \n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Layer: {layer.__class__.__name__} has no or invalid 'grad' attribute.\")\n",
    "        i += 1\n",
    "    return None\n",
    "        \n",
    "    \n",
    "def Train_Computational_Graph_NN(data_point, label, layer_num, epochs = 55, tol = 1e-5, mini_batch_size = 10):\n",
    "    x_size = data_point.shape[1]\n",
    "    num_batches = len(data_point) // mini_batch_size\n",
    "    prev_avg_error = 9999\n",
    "\n",
    "    # Building the neural network\n",
    "    network = Neural_Network()\n",
    "    for i in range(layer_num):\n",
    "        if i == 0:\n",
    "            output_size = int(input(f\"Enter output size for layer {i + 1}: \"))\n",
    "            network.add_layer(Layer(i, input_size=x_size, output_size=output_size))\n",
    "        else:\n",
    "            input_size = output_size\n",
    "            output_size = int(input(f\"Enter output size for layer {i + 1}: \"))\n",
    "            network.add_layer(Layer(i, input_size=input_size, output_size=output_size))\n",
    "\n",
    "    print(\"\\nTraining the dataset...\")\n",
    "    for epoch in range(epochs):\n",
    "        shuffled_indices = np.random.permutation(len(data_point))\n",
    "        shuffled_data = data_point[shuffled_indices]\n",
    "        shuffled_labels = label[shuffled_indices]\n",
    "        avg_sum_error = 0 # Compute the average squared error for the epoch\n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * mini_batch_size\n",
    "            end_idx = start_idx + mini_batch_size\n",
    "            mini_batch_data = shuffled_data[start_idx:end_idx]\n",
    "            mini_batch_labels = shuffled_labels[start_idx:end_idx]\n",
    "\n",
    "            for x, y in zip(mini_batch_data, mini_batch_labels):\n",
    "                desired_output = int(y)\n",
    "                one_hot_encoded = np.zeros(3).T  # Create an array of zeros of length num_classes\n",
    "                one_hot_encoded[desired_output - 1] = 1\n",
    "                #print(one_hot_encoded)\n",
    "                x_input = np.array(x).T\n",
    "                y_output = network.forward_pass(x_input) # Here is the output of A3 from the self.forward_propagation\n",
    "                #print(\"output:\")\n",
    "                #print(y_output)\n",
    "                loss = one_hot_encoded - y_output\n",
    "                #print(\"loss\")\n",
    "                #print(loss)\n",
    "                sse_error = (1/2)*np.sum(loss ** 2) # Compute sum of squares error\n",
    "                avg_sum_error += sse_error # Summation of the sum of squares error per epoch\n",
    "                network.backward_pass(one_hot_encoded) # We do back propagation using the output\n",
    "                #print_results(network)\n",
    "        \n",
    "        # Compute Average Error per epoch\n",
    "        avg_error = (1/len(shuffled_data))*avg_sum_error\n",
    "\n",
    "        #print average error per epoch\n",
    "        print(f\"Epoch {epoch + 1}: \", \"average error: \", avg_error)\n",
    "\n",
    "\n",
    "        # Check if the change in error is smaller than the tolerance (tol)\n",
    "        if abs(prev_avg_error - avg_error) < tol:\n",
    "            print(f\"Converged. Stopping training for epoch {epoch}, average error: \", avg_error)\n",
    "            break\n",
    "\n",
    "        prev_avg_error = avg_error  # Update previous error for the next iteration \n",
    "\n",
    "    print_results(network)\n",
    "    return network\n",
    "\n",
    "def import_data_set(File_dir):\n",
    "    # Initialize labels and features to store data\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "    # Read the CSV file\n",
    "    with open(File_dir, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            labels.append(float(row[0])) \n",
    "            features.append([float(x) for x in row[1:]]) \n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    np.random.seed(42)  # For reproducibility, set seed if desired\n",
    "    shuffled_indices = np.random.permutation(len(labels))\n",
    "    features = features[shuffled_indices]\n",
    "    labels = labels[shuffled_indices]\n",
    "\n",
    "    # Get unique class labels and their counts\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    # Calculate the number of samples for each class in the training set\n",
    "    train_samples_per_class = int(0.8 * len(labels) / len(unique_labels))\n",
    "\n",
    "    # Initialize arrays to store indices for training and test sets\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    # Split the indices for each class into training and test sets\n",
    "    for lbl in unique_labels:\n",
    "        indices = np.where(labels == lbl)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        train_indices.extend(indices[:train_samples_per_class])\n",
    "        test_indices.extend(indices[train_samples_per_class:])\n",
    "\n",
    "    # Convert indices to NumPy arrays\n",
    "    train_indices = np.array(train_indices)\n",
    "    test_indices = np.array(test_indices)\n",
    "\n",
    "    # Split the dataset into training and test sets based on indices\n",
    "    train_data = features[train_indices]\n",
    "    train_labels = labels[train_indices]\n",
    "\n",
    "    test_data = features[test_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "\n",
    "File_dir = r'C:\\Users\\Jeryl Salas\\OneDrive\\Documents\\AI 211 Computational Linear Algebra\\Stochastic Gradient Descent\\Iris_Dataset.csv'  \n",
    "layer_num = int(input(\"How many layers do you want in your neural network: \"))\n",
    "tr_d, tr_l, ts_d, ts_l = import_data_set(File_dir)\n",
    "network = Train_Computational_Graph_NN(tr_d, tr_l, layer_num)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

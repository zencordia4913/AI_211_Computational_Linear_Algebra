{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "processing data...\n",
      "\n",
      "Length of source vocabukary. 4146\n",
      "Maximum index in source sentences: 4146\n",
      "Length of target vocabulary. 4171\n",
      "Maximum index in target sentences: 4171\n",
      "saving processed data on .txt files\n"
     ]
    }
   ],
   "source": [
    "# PROCESS A: LOAD DATA\n",
    "# Step A1: Load the Data\n",
    "print(\"loading data...\")\n",
    "with open(r'C:\\Users\\Jeryl Salas\\OneDrive\\Documents\\AI 211 Computational Linear Algebra\\transformers\\group1_nature.tsv', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Step A2: Preprocess the Data\n",
    "print(\"processing data...\")\n",
    "data = [line.strip().split('\\t') for line in lines]\n",
    "source_sentences, target_sentences = zip(*data)\n",
    "source_sentences = [sentence.lower() for sentence in source_sentences]\n",
    "target_sentences = [sentence.lower() for sentence in target_sentences]\n",
    "\n",
    "# Step A3: Tokenization (using a simple split for demonstration)\n",
    "source_sentences_2 = [sentence.split() for sentence in source_sentences]\n",
    "target_sentences_2 = [sentence.split() for sentence in target_sentences]\n",
    "\n",
    "# Step A4: Create Vocabulary\n",
    "source_vocab = {word: idx + 1 for idx, word in enumerate(set(np.concatenate(source_sentences_2)))}\n",
    "target_vocab = {word: idx + 1 for idx, word in enumerate(set(np.concatenate(target_sentences_2)))}\n",
    "\n",
    "\n",
    "# Step A5: Numericalize the Data\n",
    "source_sentences_numeric = [\n",
    "    [source_vocab[word] for word in sentence] for sentence in source_sentences_2\n",
    "]\n",
    "target_sentences_numeric = [\n",
    "    [target_vocab[word] for word in sentence] for sentence in target_sentences_2\n",
    "]\n",
    "\n",
    "print(\"\\nLength of source vocabukary.\", len(source_vocab))\n",
    "print(\"Maximum index in source sentences:\", max(np.concatenate(source_sentences_numeric)))\n",
    "print(\"Length of target vocabulary.\", len(target_vocab))\n",
    "print(\"Maximum index in target sentences:\", max(np.concatenate(target_sentences_numeric)))\n",
    "\n",
    "\n",
    "# Step A6: Padding and splitting\n",
    "max_length = max(max(len(sentence) for sentence in source_sentences_numeric),\n",
    "                max(len(sentence) for sentence in target_sentences_numeric))\n",
    "source_sentences_padded = [sentence + [0] * (max_length - len(sentence)) for sentence in source_sentences_numeric]\n",
    "target_sentences_padded = [sentence + [0] * (max_length - len(sentence)) for sentence in target_sentences_numeric]\n",
    "split_idx = int(len(source_sentences_padded) * 0.9)\n",
    "train_source, test_source = source_sentences_padded[:split_idx], source_sentences_padded[split_idx:]\n",
    "train_target, test_target = target_sentences_padded[:split_idx], target_sentences_padded[split_idx:]\n",
    "output_folder = r'C:\\Users\\Jeryl Salas\\OneDrive\\Documents\\AI 211 Computational Linear Algebra\\transformers'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Step A8: Save Processed Data and Vocabularies\n",
    "print(\"saving processed data on .txt files\")\n",
    "np.savetxt(os.path.join(output_folder, 'train_source.txt'), train_source, fmt='%d')\n",
    "np.savetxt(os.path.join(output_folder, 'train_target.txt'), train_target, fmt='%d')\n",
    "np.savetxt(os.path.join(output_folder, 'test_source.txt'), test_source, fmt='%d')\n",
    "np.savetxt(os.path.join(output_folder, 'test_target.txt'), test_target, fmt='%d')\n",
    "np.save(os.path.join(output_folder, 'source_vocab.npy'), source_vocab)\n",
    "np.save(os.path.join(output_folder, 'target_vocab.npy'), target_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS B: DEFINE HYPERPARAMETERS\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "batch_size = 64\n",
    "max_sequence_length = max(len(seq) for seq in train_source)\n",
    "ffn_hidden = 2048\n",
    "num_layers = 5\n",
    "vocab_size = len(source_vocab) + 1\n",
    "tag_vocab_size = vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS C: DEFINE FUNCTIONS\n",
    "# STEP C1: Choose what device to use\n",
    "def choose_device():\n",
    "    if torch.cuda.is_available():\n",
    "        #print(\"We will use cuda...\")\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        #print(\"We will use cpu...\")\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "# STEP C2: Computation functions\n",
    "def linear_act_funct(input_tensor, weight, bias=None):\n",
    "    output_tensor = torch.matmul(input_tensor, weight.t())\n",
    "    if bias is not None:\n",
    "        output_tensor += bias\n",
    "    return output_tensor\n",
    "\n",
    "\n",
    "def scal_dot_prod(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1).expand(scaled.size(0), scaled.size(1), -1, -1)\n",
    "        scaled = scaled + mask\n",
    "\n",
    "    attention_raw = scaled - torch.max(scaled, dim=-1, keepdim=True)[0]\n",
    "    attention = torch.exp(attention_raw) / torch.sum(torch.exp(attention_raw), dim=-1, keepdim=True)\n",
    "\n",
    "    values = torch.matmul(attention, v)\n",
    "\n",
    "    return values, attention\n",
    "\n",
    "def relu_act_funct(input_tensor):\n",
    "    return torch.max(input_tensor, torch.zeros_like(input_tensor))\n",
    "\n",
    "def softmax_act_funct(logits, dim=-1):\n",
    "    exp_logits = torch.exp(logits - torch.max(logits, dim, keepdim=True).values)\n",
    "    sum_exp_logits = exp_logits.sum(dim, keepdim=True)\n",
    "    softmax_result = exp_logits / sum_exp_logits\n",
    "    return softmax_result\n",
    "\n",
    "def dropout(input_tensor, p=0.1):\n",
    "    mask = (torch.rand_like(input_tensor) > p).float()\n",
    "    output_tensor = input_tensor * mask / (1 - p)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "def cross_entropy_loss(logits, targets, ignore_index=None):\n",
    "    probabilities = softmax_act_funct(logits, dim=-1)\n",
    "    log_probabilities = torch.log(torch.clamp(probabilities, min=1e-20))\n",
    "    loss = -log_probabilities.gather(dim=-1, index=targets.unsqueeze(-1))\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        mask = targets != ignore_index\n",
    "        loss = loss[mask]\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS D: FUCNTIONS FOR EMBEDDINGS\n",
    "# STEP D1: INPUT EMBEDDING\n",
    "class Input_Embedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, language_to_index, start, end, padding):\n",
    "        super().__init__()\n",
    "        self.vocab_size = len(language_to_index)\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
    "        self.language_to_index = language_to_index\n",
    "        self.position_encoder = pos_encode(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.padding = padding\n",
    "    \n",
    "    def batch_tokenize(self, batch, start, end):\n",
    "\n",
    "        def tokenize(sentence, start, end):\n",
    "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
    "            if start:\n",
    "                sentence_word_indicies.insert(0, self.language_to_index[self.start])\n",
    "            if end:\n",
    "                sentence_word_indicies.append(self.language_to_index[self.end])\n",
    "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
    "                sentence_word_indicies.append(self.language_to_index[self.padding])\n",
    "            return torch.tensor(sentence_word_indicies)\n",
    "\n",
    "        tokenized = []\n",
    "        for sentence_num in range(len(batch)):\n",
    "           tokenized.append( tokenize(batch[sentence_num], start, end) )\n",
    "        tokenized = torch.stack(tokenized)\n",
    "        return tokenized.to(choose_device())\n",
    "    \n",
    "    def forward(self, x, start, end):\n",
    "        x = self.batch_tokenize(x, start, end)\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(choose_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n",
    "\n",
    "# STEP D2: POSITIONAL EMBEDDING\n",
    "class pos_encode(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS E: FUNCTIONS FOR ATTENTION\n",
    "# STEP E1: MULTI-HEAD ATTENTION\n",
    "class multi_head_attention(torch.nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_weight = torch.nn.Parameter(torch.randn(3 * d_model, d_model))\n",
    "        self.qkv_bias = torch.nn.Parameter(torch.zeros(3 * d_model))\n",
    "        self.linear_weight = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.linear_bias = torch.nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, max_sequence_length, d_model = x.size()\n",
    "        \n",
    "        # Manually compute qkv using the learned weights and biases\n",
    "        qkv = linear_act_funct(x.view(-1, d_model), self.qkv_weight, self.qkv_bias)\n",
    "        qkv = qkv.view(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        \n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scal_dot_prod(q, k, v, mask)\n",
    "        values = values.view(batch_size, max_sequence_length, self.num_heads * self.head_dim)\n",
    "        out = linear_act_funct(values.view(-1, self.num_heads * self.head_dim), self.linear_weight, self.linear_bias)\n",
    "        out = out.view(batch_size, max_sequence_length, self.d_model)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# STEP E2: MULTI-HEAD CROSS ATTENTION\n",
    "class multi_head_cross_attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.kv_weight = torch.nn.Parameter(torch.randn(2 * d_model, d_model))\n",
    "        self.kv_bias = torch.nn.Parameter(torch.zeros(2 * d_model))\n",
    "        \n",
    "        self.q_weight = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.q_bias = torch.nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "        self.linear_weight = torch.nn.Parameter(torch.randn(d_model, d_model))\n",
    "        self.linear_bias = torch.nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "\n",
    "        kv = linear_act_funct(x.view(-1, d_model), self.kv_weight, self.kv_bias)\n",
    "        kv = kv.view(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "\n",
    "        q = linear_act_funct(y.view(-1, d_model), self.q_weight, self.q_bias)\n",
    "        q = q.view(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        values, attention = scal_dot_prod(q, k, v, mask)\n",
    "        values = values.view(batch_size, sequence_length, d_model)\n",
    "\n",
    "        out = linear_act_funct(values.view(-1, d_model), self.linear_weight, self.linear_bias)\n",
    "        out = out.view(batch_size, sequence_length, d_model)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS F: NORMALIZATION OF LAYERS\n",
    "class normalization(torch.nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "\n",
    "        mean = torch.mean(inputs, dim=dims, keepdim=True)\n",
    "        var = torch.mean((inputs - mean) ** 2, dim=dims, keepdim=True)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS G: FEEDFORWARD\n",
    "class feed_forward(torch.nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(feed_forward, self).__init__()\n",
    "        self.linear1_weight = torch.nn.Parameter(torch.rand((hidden, d_model)))\n",
    "        self.linear1_bias = torch.nn.Parameter(torch.rand((hidden,)))\n",
    "        self.linear2_weight = torch.nn.Parameter(torch.rand((d_model, hidden)))\n",
    "        self.linear2_bias = torch.nn.Parameter(torch.rand((d_model,)))\n",
    "        self.dropout_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear1_output = linear_act_funct(x, self.linear1_weight, self.linear1_bias)\n",
    "        x = relu_act_funct(linear1_output)\n",
    "        x = dropout(x, p=self.dropout_prob)\n",
    "\n",
    "        linear2_output = linear_act_funct(x, self.linear2_weight, self.linear2_bias)\n",
    "        return linear2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS H: SEQUENTIAL\n",
    "class sequential_encoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class sequential_decoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS I: LAYER\n",
    "class layer_decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(layer_decoder, self).__init__()\n",
    "        self.self_attention = multi_head_attention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = normalization(parameters_shape=[d_model])\n",
    "        \n",
    "        self.encoder_decoder_attention = multi_head_cross_attention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = normalization(parameters_shape=[d_model])\n",
    "\n",
    "        self.ffn = feed_forward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = normalization(parameters_shape=[d_model])\n",
    "        self.manual_drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        _y = y.clone()\n",
    "        \n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = dropout(y, p=self.manual_drop_prob)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = dropout(y, p=self.manual_drop_prob)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.ffn(y)\n",
    "        y = dropout(y, p=self.manual_drop_prob)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class layer_encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(layer_encoder, self).__init__()\n",
    "        self.attention = multi_head_attention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = normalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = feed_forward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = normalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS J: encoder & decoder   \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index,start, end, padding):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = Input_Embedding(max_sequence_length, d_model, language_to_index, start, end, padding)\n",
    "        self.layers = sequential_encoder(*[layer_encoder(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask, start, end):\n",
    "        x = self.sentence_embedding(x, start, end)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, language_to_index, start, end, padding):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = Input_Embedding(max_sequence_length, d_model, language_to_index, start, end, padding)\n",
    "        self.layers = sequential_decoder(*[layer_decoder(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start, end):\n",
    "        y = self.sentence_embedding(y, start, end)\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "# PROCESS K: transformer  \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob, num_layers,max_sequence_length, tag_vocab_size, english_to_index, tagalog_to_index, start, end, padding):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, start, end, padding)\n",
    "        self.decoder = decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, tagalog_to_index, start, end, padding)\n",
    "        self.linear = nn.Linear(d_model, tag_vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, x, y, encoder_self_attention_mask=None, decoder_self_attention_mask=None, decoder_cross_attention_mask=None, enc_start=False, enc_end=False, dec_start=False, dec_end=False): # x, y are batch of sentences\n",
    "        x = self.encoder(x, encoder_self_attention_mask, start=enc_start, end=enc_end)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start=dec_start, end=dec_end)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS L: \n",
    "\n",
    "start = '<start>'\n",
    "padding = '<pad>'\n",
    "end = '<end>'\n",
    "\n",
    "tagalog_vocabulary = [start, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                      ':', '<', '=', '>', '?', '@',\n",
    "                      '[', '\\\\', ']', '^', '_', '`', \n",
    "                      'a', 'ã', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                      'm', 'n', 'ñ', 'ng', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                      'y', 'z', \n",
    "                      '{', '|', '}', '~', padding, end]\n",
    "\n",
    "english_vocabulary = [start, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                        ':', '<', '=', '>', '?', '@',\n",
    "                        '[', '\\\\', ']', '^', '_', '`', \n",
    "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                        'y', 'z', \n",
    "                        '{', '|', '}', '~', padding, end]\n",
    "     \n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}\n",
    "tagalog_to_index = {v:k for k,v in enumerate(tagalog_vocabulary)}\n",
    "index_to_tagalog = {k:v for k,v in enumerate(tagalog_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "\n",
    "\n",
    "d_model = 256\n",
    "batch_size = 30\n",
    "ffn_hidden = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 200\n",
    "tag_vocab_size = len(tagalog_vocabulary) + 1\n",
    "\n",
    "transformer = Transformer(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, tag_vocab_size, english_to_index, tagalog_to_index, start, end, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 12478\n",
      "Number of valid sentences: 12475\n"
     ]
    }
   ],
   "source": [
    "# PROCESS M: \n",
    "def valid_tokens(sentence, vocab):\n",
    "    for token in list(set(sentence)):\n",
    "        if token not in vocab:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def valid_length(sentence, max_sequence_length):\n",
    "    return len(list(sentence)) < (max_sequence_length - 1) \n",
    "\n",
    "valid_sentence_indicies = []\n",
    "for index in range(len(target_sentences)):\n",
    "    tagalog_sentence, english_sentence = target_sentences[index], source_sentences[index]\n",
    "    if valid_length(tagalog_sentence, max_sequence_length) \\\n",
    "      and valid_length(english_sentence, max_sequence_length) \\\n",
    "      and valid_tokens(tagalog_sentence, tagalog_vocabulary):\n",
    "        valid_sentence_indicies.append(index)\n",
    "\n",
    "print(f\"Number of sentences: {len(target_sentences)}\")\n",
    "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")\n",
    "\n",
    "target_sentences = [target_sentences[i] for i in valid_sentence_indicies]\n",
    "source_sentences = [source_sentences[i] for i in valid_sentence_indicies]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('animals are sad when locked up in a cage.', 'animals cannot live in the city.', 'animals possibly have souls.', 'animals that float are already dead.', 'animals that float are dead.', 'ants are strong.', 'ants can carry things fifty times their own weight.', 'ants eat bread.', 'ants eat dead animals.', 'ants eat sugar.', 'ants have queens.', 'ants have six legs.', 'ants have soldiers.', 'ants have workers.', 'ants like sweet food.', 'ants live underground.', 'apples are delicious.', 'apples are either colored red or green.', 'avocadoes are green.', 'bamboos are sturdy.', 'bamboos could store water.', 'banana peels are slippery.', 'banana trees are not actually trees.', 'banana trees are not sturdy.', 'banana trees have large leaves.', 'bananas are nutritious.', 'bananas are tasty.', 'bat wings are flexible.', 'bats are blind.', 'bats like to eat fruits.'), ('malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.', 'hindi kayang mamuhay ng mga hayop sa siyudad.', 'maaaring may kaluluwa ang mga hayop.', 'ang mga lumulutang na hayop ay patay na.', 'ang mga lumulutang na hayop ay patay.', 'malalakas ang mga langgam.', 'kaya ng mga langgam na bumuhat ng limampung beses ng kanilang bigat.', 'kumakain ng tinapay ang mga langgam.', 'kumakain ng mga patay na hayop ang mga langgam.', 'kumakain ng asukal ang mga langgam.', 'mayroong mga reyna ang mga langgam.', 'may anim na paa ang mga langgam.', 'mayroong mga sundalo ang mga langgam.', 'mayroong mga trabahador ang mga langgam.', 'mahilig sa matamis na pagkain ang mga langgam.', 'nakatira sa ilalim ng lupa ang mga langgam.', 'masarap ang mga mansanas.', 'ang mga mansanas ay pula o berde.', 'berde ang mga abokado.', 'matibay ang mga kawayan.', 'nakakaipon ng tubig ang mga kawayan.', 'madulas ang mga balat ng saging.', 'hindi talaga puno ang mga puno ng saging.', 'hindi matibay ang mga puno ng saging.', 'malaki ang mga dahon ng mga puno ng saging.', 'masustansya ang mga saging.', 'malasa ang mga saging.', 'nababaluktot ang mga pakpak ng paniki.', 'bulag ang mga paniki.', 'mahilig kumain ng prutas ang mga paniki.')]\n",
      "[('bats live in caves.', 'bears are studious.', 'bears are stupid.', 'bears are tough opponents.', 'bears cannot drive cars.', 'bears could dig.', 'bears eat fish.', 'bears eat people.', 'bears fight cats.', 'bears have no brains.', 'bears like honey.', 'bears like to talk.', 'bears live in the forest.', 'bears run faster than humans.', 'bears should not be disturbed.', 'bears sleep in caves.', 'bears store fat in their bodies.', 'bears talk.', 'bees are colored yellow.', 'bees are dangerous when disturbed.', 'bees attack people.', 'bees could be poisonous.', 'bees fly over flowers.', 'bees have queens similar to ants.', 'bees have queens.', 'bees like flowers.', 'bees produce honey.', 'bees protect their queen.', 'bees should not be disturbed.', 'beware of bears in the forest.'), ('nakatira sa mga kuweba ang mga paniki.', 'palaaral ang mga oso.', 'tanga ang mga oso.', 'mahirap na mga kalaban ang mga oso.', 'hindi kayang magmaneho ng kotse ang mga oso.', 'kayang maghukay ng mga oso.', 'kumakain ng mga isda ang mga oso.', 'kumakain ng mga tao ang mga oso.', 'linalabanan ng mga oso ang mga pusa.', 'walang utak ang mga oso.', 'mahilig sa pulot ang mga oso.', 'mahilig magsalita ang mga oso.', 'nakatira sa gubat ang mga oso.', 'mas mabilis tumakbo ang mga oso kaysa sa mga tao.', 'hindi dapat ginagambala ang mga oso.', 'natutulog sa kuweba ang mga oso.', 'nagiipon ng taba sa loob ng kanilang katawan ang mga oso.', 'nagsasalita ang mga oso.', 'kulay dilaw ang mga bubuyog.', 'delikado ang mga bubuyog kapag ginambala.', 'umaatake ng mga tao ang mga bubuyog.', 'posibleng makamandag ang mga bubuyog.', 'lumilipad sa taas ng bulaklak ang mga bubuyog.', 'tulad sa mga langgam, may mga reyna ang mga bubuyog.', 'may mga reyna ang mga bubuyog.', 'mahilig sa mga bulaklak ang mga bubuyog.', 'gumagawa ng pulot ang mga bubuyog.', 'pinagtatanggol ng mga bubuyog ang kanilang reyna.', 'hindi dapat ginagambala ang mga bubuyog.', 'mag-ingat sa mga oso sa gubat.')]\n",
      "[('beware of bears.', 'birds are talkative.', 'birds can fly.', 'birds can sing.', 'birds can talk.', 'birds cause danger to airplanes.', 'birds come from eggs.', 'birds could fly.', 'birds eat seeds.', 'birds have feathers.', 'bulls are aggressive.', 'bulls are mad at the color red.', 'bulls have sharp horns.', 'bulls kill people.', 'butterflies are hard to catch.', 'cabbages are green.', 'cabbages are tasteless.', \"cat's teeth are small.\", 'cats are active at night.', 'cats are arrogant.', 'cats are mysterious.', 'cats are not as loyal as dogs.', 'cats are selfish.', 'cats are strong.', 'cats are visiting the cemetery.', 'cats can see in the dark.', 'cats do cry.', 'cats do not cry.', 'cats have nine lives.', 'cats like milk.'), ('mag-ingat sa mga oso.', 'madaldal ang mga ibon.', 'kayang lumipad ng mga ibon.', 'kayang kumanta ng mga ibon.', 'kayang magsalita ng mga ibon.', 'nagdudulot ng panganib ang mga ibon sa mga eroplano.', 'nanggagaling sa mga itlog ang mga ibon.', 'nakakalipad ang mga ibon.', 'kumakain ng mga buto ang mga ibon.', 'mayroong mga balahibo ang mga ibon.', 'agresibo ang mga toro.', 'galit sa kulay pula ang mga toro.', 'mayroong mga matulis na sungay ang mga toro.', 'pumapatay ng mga tao ang mga toro.', 'mahirap hulihin ang mga paruparo.', 'berde ang mga repolyo.', 'walang lasa ang mga repolyo.', 'maliit ang mga ngipin ng pusa.', 'aktibo sa gabi ang mga pusa.', 'arogante ang mga pusa.', 'mahiwaga ang mga pusa.', 'hindi kasing tapat ng mga aso ang mga pusa.', 'madamot ang mga pusa.', 'malalakas ang mga pusa.', 'bumibisita ng sementeryo ang mga pusa.', 'nakakakita sa dilim ang mga pusa.', 'umiiyak ang mga pusa.', 'hindi umiiyak ang mga pusa.', 'may siyam na buhay ang mga pusa.', 'mahilig sa gatas ang mga pusa.')]\n",
      "[('cats like the microphone.', 'cats love kittens.', 'cats love pineapples.', 'cats love to bow.', 'cats love to fire arrows.', 'cats love to hide.', 'cats prefer to be alone.', 'cats rarely cry.', 'cats, when angered, are evil.', 'chicks are yellow.', 'clams and oysters are different.', 'coconut oil could be used in cooking.', 'coconuts are bad.', 'coconuts are hard.', 'coconuts are plentiful in the philippines.', 'coconuts are plentiful.', 'coconuts grow on coconut trees.', 'cows eat grass.', 'cows have kidneys.', 'cows have multiple stomachs.', 'cows have teeth made for chewing.', 'cows produce milk, which could be made into cheese.', 'cows produce milk.', 'crabs are usually colored red.', 'crabs could be as large as thirteen feet across.', 'crabs have shells.', 'crabs have ten legs.', 'crabs walk sideways.', 'dead plants become coal.', 'dead plants will become coal.'), ('gusto ng mga pusa ang mikropono.', 'mahilig sa mga kuting ang mga pusa.', 'mahilig sa mga pinya ang mga pusa.', 'mahilig yumuko ang mga pusa.', 'mahilig tumira ng mga pana ang mga pusa.', 'mahilig magtago ang mga pusa.', 'mas gusto ng mga pusa na mag-isa.', 'bihirang umiyak ang mga pusa.', 'ang mga pusa, kapag ginalit, ay masama.', 'dilaw ang mga sisiw.', 'magkaiba ang mga kabibe at mga talaba.', 'puwedeng gamitin sa pagluto ang mantika ng buko.', 'masama ang mga buko.', 'matigas ang mga buko.', 'masagana ang mga buko sa pilipinas.', 'masagana ang mga buko.', 'tumutubo ang mga buko sa mga puno ng buko.', 'kumakain ng damo ang mga baka.', 'may mga bato ang mga baka.', 'may madaming tiyan ang mga baka.', 'may mga ngiping gawa para ngumuya ang mga baka.', 'gumagawa ng gatas ang mga baka, na puwedeng gawing keso.', 'gumagawa ng gatas ang mga baka.', 'madalas ay kulay pula ang mga alimango.', 'puwedeng umabot sa labing-tatlong talampakang pahalang ang isang alimango.', 'may mga balat ang mga alimango.', 'mayroong sampung paa ang mga alimango.', 'naglalakad nang patagilid ang mga alimango.', 'ang mga patay na halaman ay nagiging uling.', 'ang mga patay na halaman ay magiging uling.')]\n",
      "[('deers are friendly animals.', 'deers want justice.', 'dogs and cats are friends.', 'dogs are expensive to feed.', 'dogs are expensive to take care of.', 'dogs are helpful.', 'dogs are loyal to their master.', 'dogs are loyal to their masters.', 'dogs are loyal.', 'dogs eat vegetables in the room.', 'dogs eat vegetables in the room.', 'dogs evolved from wolves.', 'dogs have a strong sense of smell.', 'dogs have intestines.', 'dogs like to bark.', 'dogs live near humans.', 'dogs rarely eat.', 'dogs will eat anything.', 'dogs, cats, and rats are types of animals.', 'dogs, when taken care of, are friendly.', 'doves and crows are mortal enemies.', 'dried leaves are not colored green.', 'ducks and geese look alike.', 'ducks are beautiful.', 'elephants are scared of mice.', 'elephants are scared of rats.', 'elephants can paint.', 'female lions are better than male lions.', 'female lions are protective of their young.', 'fireflies are eaten by crocodiles.'), ('palakaibigan na hayop ang mga usa.', 'gusto ng mga usa ng hustisya.', 'magkaibigan ang mga aso at pusa.', 'mahal pakainin ang mga aso.', 'mahal alagaan ang mga aso.', 'matulungin ang mga aso.', 'tapat ang mga aso sa kanilang amo.', 'tapat ang mga aso sa kanilang mga amo.', 'tapat ang mga aso.', 'kumakain ng mga gulay ang aso sa kwarto.', 'kumakain ng mga gulay sa kwarto ang aso.', 'galing sa mga lobo ang mga aso.', 'mayroong malakas na pangamoy ang mga aso.', 'may mga bituka ang mga aso.', 'mahilig tumahol ang mg aso.', 'nakatira malapit sa mga tao ang mga aso.', 'bihirang kumain ang mga aso.', 'kakain ng kahit ano ang mga aso.', 'ang mga aso, pusa, at daga ay mga uri ng hayop.', 'ang mga aso, kapag inalagaan, ay palakaibigan.', 'mortal na magkaaway ang mga kalapati at ang mga uwak.', 'hindi kulay berde ang mga natuyong mga dahon.', 'magkamukha ang mga gansa at ang mga pato.', 'maganda ang mga pato.', 'takot sa mga daga ang mga elepante.', 'takot sa mga daga ang mga elepante.', 'kayang magpinta ng mga elepante.', 'mas magaling kaysa sa mga lalaking liyon ang mga babaing liyon.', 'mapagtanggol sa kanilang mga bata ang mga babaing liyon.', 'kinakain ng mga buwaya ang mga alitaptap.')]\n"
     ]
    }
   ],
   "source": [
    "# PROCESS N: \n",
    "class text_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, source_sentences, target_sentences):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_sentences[idx], self.target_sentences[idx]\n",
    "    \n",
    "dataset = text_dataset(source_sentences, target_sentences)\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size)\n",
    "iterator = iter(train_loader)\n",
    "\n",
    "for batch_num, batch in enumerate(iterator):\n",
    "    print(batch)\n",
    "    if batch_num > 3:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS O: \n",
    "n_inf = -1e9\n",
    "\n",
    "def create_masks(eng_batch, tag_batch):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      eng_sentence_length, tag_sentence_length = len(eng_batch[idx]), len(tag_batch[idx])\n",
    "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
    "      tag_chars_to_padding_mask = np.arange(tag_sentence_length + 1, max_sequence_length)\n",
    "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, tag_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_self_attention[idx, tag_chars_to_padding_mask, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "      decoder_padding_mask_cross_attention[idx, tag_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, n_inf, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, n_inf, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, n_inf, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 : 5.0695977210998535\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: }}\\#}@}}11-#1}@}1--#11n-}n\"n>nnn>>>>nnnniknqq7q@@q97ngng 11ng_0_@ngngng7ngng10>ng@0ng<pad>=ñn=<pad>nnnaa<pad>aqnq<q00qnqaa2#2>20&22ñn2ngn>@0ngnng<pad>nnqnh-ng<pad>q2ng2<pad>ng22222@222222222q22nn2n22000111nn22020ng-b227b7ngng\\ng\\\\\\27wng)ngngngng77wng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 2.4864444732666016\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: maanann gng amgangaaaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 2.273855686187744\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: mataa n agg aaag.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.3618502616882324\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: nnaaaia agag naa aag aaga aangaaangang aanagaaa\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 2.256173610687256\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: nana ang aa   aa  ang kananagg namang  \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 1:  2.5184568499143305\n",
      "Epoch 1\n",
      "Iteration 0 : 2.217374563217163\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: mana g  nan g aa naananaaanan ng anana aakng aa nnna aannnang aanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.9408546686172485\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: mananananng aasa.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.8650381565093994\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: manaa   ang aaag.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.1513671875\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: nnanamg anananaagaag sanananng aangang aananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 2.114171266555786\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: nana ang aan  aaa ang aanana g mo ongng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 2:  2.1042785231883707\n",
      "Epoch 2\n",
      "Iteration 0 : 2.057692050933838\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: na amg an n g aa naanananana  ng anana g n g ma nn a aa nnang aanaa \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.7657790184020996\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: malal n nng mana.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.8324098587036133\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: malakan ang aamg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.1387174129486084\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: nnanang anananaanaag nana aang aangang aananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 2.047858476638794\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: nana ang a n  aoa ang ma a ang aa obg g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 3:  2.0108138789924292\n",
      "Epoch 3\n",
      "Iteration 0 : 1.9836586713790894\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: nagimg anaang aa naananamagan agna anana ang na nnaa nananana ninaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.6905120611190796\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: malalan nng man..g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.745233178138733\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: malaaa  ang masg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.122875928878784\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: ananaag anananaa aagnnananaang aangang nananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 2.0211963653564453\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: mana ang aan  aaa a g na a a   manangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 4:  1.9543909742855108\n",
      "Epoch 4\n",
      "Iteration 0 : 1.9425119161605835\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: manamg a anng na nnana amanan ng a anang nng ma nnnanaannnan. aanaa \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.6332303285598755\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: malal n ang mala.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.6508277654647827\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: balama  ang mamg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.095811605453491\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: ananaag anananaanaag aani aang nanga g aa anana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.947271704673767\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: magalong aan  aaa nng nana ang na ang  \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 5:  1.9073712802850282\n",
      "Epoch 5\n",
      "Iteration 0 : 1.9188599586486816\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: magamg a anng na nnananana a  ag anama g ang maannnanma nnan. nanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.5945284366607666\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: malal n ang mana.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.6393253803253174\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nalama  ang mamg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.049461603164673\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: anuling anananaa ang nana aa g nangang nananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.9358183145523071\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: mana a g nana aaa nng aa ana g nanangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 6:  1.8698904743561378\n",
      "Epoch 6\n",
      "Iteration 0 : 1.887745976448059\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: mayamg a  ang ma nnananaaana  ma ananang m g na nnnanma nnang aanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.5120134353637695\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: malalan ang mana.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.5698046684265137\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nalama  ang mamg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.051891803741455\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: amulaag anananaa aagnaananan gaaangang na anana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.9022725820541382\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: magama g aalaraaa nng aa i ang pa angng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 7:  1.8315120597298329\n",
      "Epoch 7\n",
      "Iteration 0 : 1.8457998037338257\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: manamg a amng na naana aaanan mgnananang nng na nnna na anang nanaa \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.5054371356964111\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makawan ang aasa..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.566455602645874\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahama  ang mamg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 2.0102925300598145\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: atuling anabanaa aag nana anng nangang na anana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.876443862915039\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: maralang agn laaa mng ba a a g panalgng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 8:  1.801494608131739\n",
      "Epoch 8\n",
      "Iteration 0 : 1.8148096799850464\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: mayamg anaang aa nnana ananan ag a anang nng maannna nannnang ninaa.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.3608243465423584\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makaman ang aana..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.4708523750305176\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahal   ang pal..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.9677176475524902\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: isanina nna anaa aag aana aang na gang nananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.8233485221862793\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: maya ang aan ngaa ang nananang nanang g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 9:  1.7349627416294355\n",
      "Epoch 9\n",
      "Iteration 0 : 1.7773480415344238\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: malimgaanaang ma naa a akana  ag anasa g nng na nnna aananang sanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.2330896854400635\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makakan ang mana..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.3636765480041504\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahila  ang pasg.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.926356315612793\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: atulimg ana anaa ang aalagaang na gang na anani\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.7699061632156372\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: marami g aa a aaa a g manana g nanangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 10:  1.6611336334966695\n",
      "Epoch 10\n",
      "Iteration 0 : 1.7152761220932007\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: malukgkkkanng ma nai a aaa an ng ananang nng ma nnnannannnang nanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.1368992328643799\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makatan ang aama..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.3142508268356323\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: lahala  ang pas..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.861510157585144\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: amumamg anananaanabgnaala  nng nanga g aa a ana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.724936842918396\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: mayamimg aam raoa bng ma anang nanangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 11:  1.609964075856484\n",
      "Epoch 11\n",
      "Iteration 0 : 1.6903609037399292\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: malingaanaang ma mmananakana angnananang kna na  nnannannna g ninaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 1.0544795989990234\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makam n ang mama.g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.2317841053009033\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahila  ang pap..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.8104963302612305\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: amamimg anana aa apgnnani  nng aa gang nananana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.650428295135498\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: maramang ma   aaa mng ma ana g ninangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 12:  1.5334443103235502\n",
      "Epoch 12\n",
      "Iteration 0 : 1.6085094213485718\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: maganaka aang ma nnana amana  ng anamang ang aa  nna.aa nnang manaa \n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 0.8769177198410034\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makamas ang mila..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 1.0658084154129028\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahula  ang pa.o.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.7201318740844727\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: alalang anata aa apgnpalinasng aa ga g na a ana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.5887253284454346\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: marami g agnarbae ang aana ang nanangng\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 13:  1.425361952672784\n",
      "Epoch 13\n",
      "Iteration 0 : 1.5499311685562134\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: malungkakaang ma naan namanan ng ananang nng na nnna.nannnang mi aan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 0.808082103729248\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makatas ang mama..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 0.9759431481361389\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahulag ang pap..\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 300 : 1.6532467603683472\n",
      "English Senence: the firefly knows its brother is alive.\n",
      "Actual Tagalog Translation: alam ng alitaptap na buhay ang kanyang kapatid.\n",
      "Tagalog Prediction: anuming anapatuababgbbusa an g na gang ka a ana\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 400 : 1.4968913793563843\n",
      "English Senence: there are a lot of members of the evil society.\n",
      "Actual Tagalog Translation: maraming miyembro ang masamang lipunan.\n",
      "Tagalog Prediction: maramang mamaraaa amg mamanang panayg g\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Average loss for epoch 14:  1.3275156372155135\n",
      "Epoch 14\n",
      "Iteration 0 : 1.4656254053115845\n",
      "English Senence: animals are sad when locked up in a cage.\n",
      "Actual Tagalog Translation: malungkot ang mga hayop kapag nakakulong ang mga ito sa isang hawla.\n",
      "Tagalog Prediction: malunak  anng mgannay paka an ngnananang ang aa nnna ninnn ng sanaan\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 100 : 0.6728951930999756\n",
      "English Senence: the lemon is juicy.\n",
      "Actual Tagalog Translation: makatas ang limon.\n",
      "Tagalog Prediction: makatas ang lamal.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
      "Iteration 200 : 0.8757729530334473\n",
      "English Senence: the tree fell.\n",
      "Actual Tagalog Translation: nahulog ang puno.\n",
      "Tagalog Prediction: nahelag ang pano.\n",
      "__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask \u001b[38;5;241m=\u001b[39m create_masks(eng_batch, tag_batch)\n\u001b[0;32m     16\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m tag_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtag_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mencoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menc_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdec_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m labels \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39msentence_embedding\u001b[38;5;241m.\u001b[39mbatch_tokenize(tag_batch, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(\n\u001b[0;32m     28\u001b[0m     tag_predictions\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tag_vocab_size)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     29\u001b[0m     labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     30\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mtagalog_to_index[padding]\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 78\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, y, encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask, enc_start, enc_end, dec_start, dec_end)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[0;32m     68\u001b[0m             x, \n\u001b[0;32m     69\u001b[0m             y, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m             dec_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# We should make this true\u001b[39;00m\n\u001b[0;32m     76\u001b[0m             dec_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m): \u001b[38;5;66;03m# x, y are batch of sentences\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, encoder_self_attention_mask, start\u001b[38;5;241m=\u001b[39menc_start, end\u001b[38;5;241m=\u001b[39menc_end)\n\u001b[1;32m---> 78\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_self_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_cross_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdec_end\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(out)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[43], line 42\u001b[0m, in \u001b[0;36mdecoder.forward\u001b[1;34m(self, x, y, self_attention_mask, cross_attention_mask, start, end)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, self_attention_mask, cross_attention_mask, start, end):\n\u001b[0;32m     41\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentence_embedding(y, start, end)\n\u001b[1;32m---> 42\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m, in \u001b[0;36msequential_decoder.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m     11\u001b[0m x, y, self_attention_mask, cross_attention_mask \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m---> 13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 20\u001b[0m, in \u001b[0;36mlayer_decoder.forward\u001b[1;34m(self, x, y, self_attention_mask, cross_attention_mask)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, self_attention_mask, cross_attention_mask):\n\u001b[0;32m     18\u001b[0m     _y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m---> 20\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     y \u001b[38;5;241m=\u001b[39m dropout(y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_drop_prob)\n\u001b[0;32m     22\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(y \u001b[38;5;241m+\u001b[39m _y)\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jeryl Salas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[38], line 31\u001b[0m, in \u001b[0;36mmulti_head_attention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     27\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#print(f\"q size: {q.size()}, k size: {k.size()}, v size: {v.size()}, \")\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Use scal_dot_prod with manual operations\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m values, attention \u001b[38;5;241m=\u001b[39m \u001b[43mscal_dot_prod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(f\"values.size(): {values.size()}, attention.size:{ attention.size()} \")\u001b[39;00m\n\u001b[0;32m     34\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mview(batch_size, max_sequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "Cell \u001b[1;32mIn[36], line 34\u001b[0m, in \u001b[0;36mscal_dot_prod\u001b[1;34m(q, k, v, mask)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Step 4: Manual softmax activation\u001b[39;00m\n\u001b[0;32m     33\u001b[0m attention_raw \u001b[38;5;241m=\u001b[39m scaled \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(scaled, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 34\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_raw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mexp(attention_raw), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Compute the weighted sum of values\u001b[39;00m\n\u001b[0;32m     37\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention, v)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROCESS P: \n",
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 20\n",
    "epoch_loss_data = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    batch_losses = []\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, tag_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, tag_batch)\n",
    "        optim.zero_grad()\n",
    "        tag_predictions = transformer(eng_batch,\n",
    "                                     tag_batch,\n",
    "                                      encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     enc_start=False,\n",
    "                                     enc_end=False,\n",
    "                                     dec_start=True,\n",
    "                                     dec_end=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(tag_batch, start=False, end=True)\n",
    "        loss = cross_entropy_loss(\n",
    "            tag_predictions.view(-1, tag_vocab_size).to(device),\n",
    "            labels.view(-1).to(device),\n",
    "            ignore_index=tagalog_to_index[padding]\n",
    "        )\n",
    "\n",
    "        valid_indicies = torch.where(labels.view(-1) == tagalog_to_index[padding], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # Calculate and store the loss for the current batch\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English Senence: {eng_batch[0]}\")\n",
    "            print(f\"Actual Tagalog Translation: {tag_batch[0]}\")\n",
    "            tag_sentence_predicted = torch.argmax(tag_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in tag_sentence_predicted:\n",
    "                if idx == tagalog_to_index[end]:\n",
    "                    break\n",
    "                predicted_sentence += index_to_tagalog[idx.item()]\n",
    "            print(f\"Tagalog Prediction: {predicted_sentence}\")\n",
    "            print(\"__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________\")\n",
    "\n",
    "\n",
    "    # Calculate and store the average loss for the current epoch\n",
    "    epoch_loss = sum(batch_losses) / len(batch_losses)\n",
    "    epoch_loss_data.append(epoch_loss)\n",
    "\n",
    "    # Printing and plotting of loss per epoch\n",
    "    print(f'Average loss for epoch {epoch + 1}: ', epoch_loss)\n",
    "    plt.plot(epoch_loss_data, label=f'Epoch {epoch + 1}')\n",
    "# Plot showing\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
